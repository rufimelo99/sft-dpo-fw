{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0845c530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sezhou/sfo-dpo-fw/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Recreate SFT step (toy) for investigator pÎ¸(x|y)\n",
    "import os, time, math, random\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# ---- Config ----\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "PM_ID  = \"gpt2-large\"   # target LM pm (forward x->y)\n",
    "INV_ID = \"gpt2-large\"   # investigator base pÎ¸ (we'll fine-tune)\n",
    "\n",
    "# Tiny prefix distribution P_SFT\n",
    "PREFIXES = [\n",
    "    \"A short note about machine learning:\",\n",
    "    \"Three tips for staying productive:\",\n",
    "    \"A gentle introduction to probability:\",\n",
    "    \"In a surprising discovery, scientists found\",\n",
    "    \"As a software engineer, I often consider\",\n",
    "    \"In Japan, the Shinkansen is known for\",\n",
    "    \"A concise summary of the book is\",\n",
    "    \"The quick brown fox\",\n",
    "    \"An explanation for beginners:\",\n",
    "    \"A brief overview of databases:\"\n",
    "]\n",
    "\n",
    "NUM_EXAMPLES   = 10     # size of DSFT\n",
    "MAX_PREFIX_EXT = 8      # optional: extend x slightly for variety\n",
    "MAX_SUFFIX_LEN = 15     # greedy suffix length\n",
    "BATCH_SIZE     = 2\n",
    "EPOCHS         = 2\n",
    "LR             = 5e-5\n",
    "WARMUP_RATIO   = 0.1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "\n",
    "# Repro\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"[INFO] Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b4d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Loading tokenizer (pm)...\n",
      "[STEP] Loading target model pm (x->y)... (first run may download)\n",
      "[STEP] Loading tokenizer (investigator pÎ¸)...\n",
      "[STEP] Loading investigator base model pÎ¸ (will fine-tune)...\n",
      "[TIME] Models loaded in 7.02s\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "print(\"[STEP] Loading tokenizer (pm)...\")\n",
    "tok_pm = AutoTokenizer.from_pretrained(PM_ID)\n",
    "if tok_pm.pad_token is None:\n",
    "    tok_pm.pad_token = tok_pm.eos_token\n",
    "\n",
    "print(\"[STEP] Loading target model pm (x->y)... (first run may download)\")\n",
    "pm = AutoModelForCausalLM.from_pretrained(\n",
    "    PM_ID,\n",
    "    use_safetensors=True,\n",
    "    device_map=\"auto\"            # âœ… automatically put on GPU\n",
    ")\n",
    "pm.eval()\n",
    "\n",
    "print(\"[STEP] Loading tokenizer (investigator pÎ¸)...\")\n",
    "tok_inv = AutoTokenizer.from_pretrained(INV_ID)\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "\n",
    "print(\"[STEP] Loading investigator base model pÎ¸ (will fine-tune)...\")\n",
    "inv = AutoModelForCausalLM.from_pretrained(\n",
    "    INV_ID,\n",
    "    use_safetensors=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"[TIME] Models loaded in {time.time()-t0:.2f}s\")\n",
    "# After loading your model\n",
    "print(next(pm.parameters()).device)\n",
    "print(next(inv.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d74fec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Building DSFT: sample x ~ P_SFT, then y â† pm(x) (sampled)\n",
      "[TIME] Built DSFT with 10 examples in 3.95s\n",
      "\n",
      "=== PRINTING DSFT (x, y) ===\n",
      "\n",
      "[1] x: Three tips for staying productive:\n",
      "[1] y: Learn to read\n",
      "\n",
      "Read books that have lots of examples and\n",
      "\n",
      "[2] x: A short note about machine learning:\n",
      "[2] y: it's not going to fix the world, because there will always be people\n",
      "\n",
      "[3] x: As a software engineer, I often consider\n",
      "[3] y: myself a good programmer (and even though I hate to admit it, I\n",
      "\n",
      "[4] x: In a surprising discovery, scientists found\n",
      "[4] y: that the bacteria's DNA is actually made up of five chromosomes, each of\n",
      "\n",
      "[5] x: In a surprising discovery, scientists found\n",
      "[5] y: that the brain's dopamine system had a higher response to nicotine than to cocaine\n",
      "\n",
      "[6] x: A gentle introduction to probability:\n",
      "[6] y: the power of statistics\n",
      "\n",
      "The power of statistics is that it allows you\n",
      "\n",
      "[7] x: Three tips for staying productive:\n",
      "[7] y: 1) Set a timer for a half hour and work for 45\n",
      "\n",
      "[8] x: An explanation for beginners:\n",
      "[8] y: When you think of a \"giant\" mushroom, the first thing that\n",
      "\n",
      "[9] x: Three tips for staying productive:\n",
      "[9] y: Work on a daily basis. Every day, you must take care\n",
      "\n",
      "[10] x: A brief overview of databases:\n",
      "[10] y: A brief overview of databases: http://www.fcd.gov/\n"
     ]
    }
   ],
   "source": [
    "def gen_from(model, tok, text, max_new_tokens, device=DEVICE):\n",
    "    \"\"\"Sample continuation from given text (avoids greedy repetition).\"\"\"\n",
    "    inputs = tok(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,           # sampling instead of greedy\n",
    "            top_p=0.95,               # nucleus sampling\n",
    "            temperature=0.8,          # soften logits\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"[STEP] Building DSFT: sample x ~ P_SFT, then y â† pm(x) (sampled)\")\n",
    "t_ds = time.time()\n",
    "pairs = []\n",
    "for i in range(NUM_EXAMPLES):\n",
    "    x = random.choice(PREFIXES)\n",
    "    full = gen_from(pm, tok_pm, x, max_new_tokens=MAX_SUFFIX_LEN)\n",
    "    y = full[len(x):].strip() if full.startswith(x) else full\n",
    "    pairs.append((x, y))\n",
    "\n",
    "print(f\"[TIME] Built DSFT with {len(pairs)} examples in {time.time()-t_ds:.2f}s\")\n",
    "print(\"\\n=== PRINTING DSFT (x, y) ===\")\n",
    "for idx, (x, y) in enumerate(pairs, 1):\n",
    "    print(f\"\\n[{idx}] x: {x}\")\n",
    "    print(f\"[{idx}] y: {y}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfb2775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train size: 8 | Val size: 2\n",
      "\n",
      "[SAMPLE TOKENS]\n",
      "input_ids len: 29\n",
      "supervised tokens: 6\n"
     ]
    }
   ],
   "source": [
    "IN_CONTEXT_PREFIX = \"Suffix:\\n\"\n",
    "MID_PROMPT = \"\\nPrefix:\\n\"\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    For each (x, y), we feed the model:\n",
    "      input:  'Suffix:\\\\n{y}\\\\nPrefix:\\\\n' + x\n",
    "      labels: supervise only on x (mask out the suffix part)\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, tokenizer, max_len=256):\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.items = []\n",
    "        for (x, y) in pairs:\n",
    "            src = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "            tgt = x\n",
    "            self.items.append((src, tgt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        src, tgt = self.items[i]\n",
    "        enc_all = self.tok(\n",
    "            src + tgt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = enc_all[\"input_ids\"][0]\n",
    "        attn = enc_all[\"attention_mask\"][0]\n",
    "\n",
    "        # mask out source portion\n",
    "        src_len = len(self.tok(src, truncation=True, max_length=self.max_len)[\"input_ids\"])\n",
    "        labels = input_ids.clone()\n",
    "        labels[:src_len] = -100\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels}\n",
    "\n",
    "def collate(batch):\n",
    "    keys = batch[0].keys()\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        seqs = [b[k] for b in batch]\n",
    "        pad_val = tok_inv.pad_token_id if k != \"labels\" else -100\n",
    "        out[k] = torch.nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_val)\n",
    "    return out\n",
    "\n",
    "# Split train/val\n",
    "cut = int(0.8 * len(pairs))\n",
    "train_pairs = pairs[:cut]\n",
    "val_pairs   = pairs[cut:]\n",
    "\n",
    "train_ds = XYDataset(train_pairs, tok_inv)\n",
    "val_ds   = XYDataset(val_pairs, tok_inv)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate)\n",
    "val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "\n",
    "print(f\"[INFO] Train size: {len(train_ds)} | Val size: {len(val_ds)}\")\n",
    "\n",
    "# Peek at tokenized structure\n",
    "sample_item = train_ds[0]\n",
    "print(\"\\n[SAMPLE TOKENS]\")\n",
    "print(\"input_ids len:\", sample_item[\"input_ids\"].shape[0])\n",
    "print(\"supervised tokens:\", (sample_item[\"labels\"] != -100).sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd4e10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Baseline check: investigator BEFORE training...\n",
      "[Baseline predictions BEFORE SFT]\n",
      "\n",
      "--- Example 1 ---\n",
      "y_true (suffix): Work on a daily basis. Every day, you must take care\n",
      "x_true (gold prefix): Three tips for staying productive:\n",
      "x_hat  (pred prefix): Work on a daily basis. Every day, you must take care\n",
      "Prefix:\n",
      "Work on a daily basis. Every day, you must take care\n",
      "\n",
      "--- Example 2 ---\n",
      "y_true (suffix): A brief overview of databases: http://www.fcd.gov/\n",
      "x_true (gold prefix): A brief overview of databases:\n",
      "x_hat  (pred prefix): Prefix:\n",
      "Prefix:\n",
      "Prefix:\n",
      "Prefix:\n",
      "Prefix:\n",
      "Prefix:\n",
      "Prefix:\n",
      "Prefix:\n",
      "\n",
      "[END baseline check]\n"
     ]
    }
   ],
   "source": [
    "print(\"[STEP] Baseline check: investigator BEFORE training...\")\n",
    "\n",
    "# Make sure padding is set correctly\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "if inv.config.pad_token_id is None:\n",
    "    inv.config.pad_token_id = tok_inv.pad_token_id\n",
    "\n",
    "def inv_predict_prefix_baseline(y: str, max_new_tokens=32):\n",
    "    prompt = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    enc = tok_inv(prompt, return_tensors=\"pt\")\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen = inv.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok_inv.pad_token_id,\n",
    "        )\n",
    "    text = tok_inv.decode(gen[0], skip_special_tokens=True)\n",
    "    return text.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in text else text\n",
    "\n",
    "print(\"[Baseline predictions BEFORE SFT]\")\n",
    "for i, (x_true, y_true) in enumerate(val_pairs[:5], 1):\n",
    "    x_hat = inv_predict_prefix_baseline(y_true, max_new_tokens=32)\n",
    "\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    print(\"y_true (suffix):\", (y_true[:100] + \"...\") if len(y_true) > 100 else y_true)\n",
    "    print(\"x_true (gold prefix):\", x_true)\n",
    "    print(\"x_hat  (pred prefix):\", x_hat)\n",
    "\n",
    "print(\"\\n[END baseline check]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76e916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Training investigator pÎ¸ to predict x from y (SFT)...\n",
      "Sanity check: supervised tokens in first batch: 12\n",
      "  [epoch 1 step 4/4] loss=3.4042\n",
      "  [epoch 2 step 4/4] loss=1.2321\n",
      "[TIME] Training done in 35.18s\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "print(\"[STEP] Training investigator pÎ¸ to predict x from y (SFT)...\")\n",
    "t_train = time.time()\n",
    "\n",
    "inv.train()\n",
    "opt = AdamW(inv.parameters(), lr=LR)   # use constant LR\n",
    "\n",
    "num_training_steps = EPOCHS * math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
    "num_warmup = int(WARMUP_RATIO * num_training_steps)\n",
    "sched = get_cosine_schedule_with_warmup(opt, num_warmup, num_training_steps)\n",
    "\n",
    "# ðŸ” sanity check: make sure labels arenâ€™t all -100\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"Sanity check: supervised tokens in first batch:\",\n",
    "      (sample_batch[\"labels\"] != -100).sum().item())\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    running = 0.0\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "        out = inv(**batch)\n",
    "        loss = out.loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if step % GRAD_ACCUM_STEPS == 0:\n",
    "            clip_grad_norm_(inv.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "            opt.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "        running += loss.item() * GRAD_ACCUM_STEPS\n",
    "        if step % 5 == 0 or step == len(train_loader):\n",
    "            print(f\"  [epoch {epoch} step {step}/{len(train_loader)}] loss={running/step:.4f}\")\n",
    "\n",
    "print(f\"[TIME] Training done in {time.time()-t_train:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ee50d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Sampling a few validations to see behavior...\n",
      "\n",
      "--- Example 1 ---\n",
      "y_true (suffix): Work on a daily basis. Every day, you must take care\n",
      "x_true (gold prefix): Three tips for staying productive:\n",
      "x_hat  (pred prefix): When you start getting good at staying productive, you can expect to spend as much as 90% of your time doing something productive.\n",
      "In a surprising discovery,\n",
      "pm continuation from x_hat: researchers at the University of Missouri-Columbia found that people who worked hard for their educa...\n",
      "\n",
      "--- Example 2 ---\n",
      "y_true (suffix): A brief overview of databases: http://www.fcd.gov/\n",
      "x_true (gold prefix): A brief overview of databases:\n",
      "x_hat  (pred prefix): A brief introduction to databases: http://www.fcd.gov/resources/resources/guide/introduction_to_databases.html\n",
      "What\n",
      "pm continuation from x_hat: makes a database great?\n",
      "1. The database must support an automated, fully automated process\n",
      "2. The da...\n",
      "\n",
      "[ALL DONE]\n"
     ]
    }
   ],
   "source": [
    "inv.eval()\n",
    "\n",
    "# Ensure pad token is set correctly\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "if inv.config.pad_token_id is None:\n",
    "    inv.config.pad_token_id = tok_inv.pad_token_id\n",
    "\n",
    "def inv_predict_prefix(y: str, max_new_tokens=32):\n",
    "    prompt = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    enc = tok_inv(prompt, return_tensors=\"pt\")\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen = inv.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok_inv.pad_token_id,   # âœ… consistent pad token\n",
    "        )\n",
    "    text = tok_inv.decode(gen[0], skip_special_tokens=True)\n",
    "    return text.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in text else text\n",
    "\n",
    "def pm_continue(x: str, max_new_tokens=40):\n",
    "    full = gen_from(pm, tok_pm, x, max_new_tokens=max_new_tokens)\n",
    "    return full[len(x):].strip() if full.startswith(x) else full\n",
    "\n",
    "print(\"[STEP] Sampling a few validations to see behavior...\")\n",
    "for i, (x_true, y_true) in enumerate(val_pairs[:5], 1):\n",
    "    x_hat = inv_predict_prefix(y_true, max_new_tokens=32)\n",
    "    y_from_hat = pm_continue(x_hat, max_new_tokens=40)\n",
    "\n",
    "    print(\"\\n--- Example\", i, \"---\")\n",
    "    print(\"y_true (suffix):\", (y_true[:100] + \"...\") if len(y_true) > 100 else y_true)\n",
    "    print(\"x_true (gold prefix):\", x_true)\n",
    "    print(\"x_hat  (pred prefix):\", x_hat)\n",
    "    print(\"pm continuation from x_hat:\", (y_from_hat[:100] + \"...\") if len(y_from_hat) > 100 else y_from_hat)\n",
    "\n",
    "print(\"\\n[ALL DONE]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb686cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Saving investigator model to inv_sft_checkpoint ...\n",
      "[DONE] Model + tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# === Save fine-tuned investigator model ===\n",
    "SAVE_DIR = \"inv_sft_checkpoint\"\n",
    "\n",
    "print(f\"[STEP] Saving investigator model to {SAVE_DIR} ...\")\n",
    "inv.save_pretrained(SAVE_DIR)\n",
    "tok_inv.save_pretrained(SAVE_DIR)\n",
    "print(\"[DONE] Model + tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48ebbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sezhou/sfo-dpo-fw/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Loading fine-tuned investigator model from inv_sft_checkpoint ...\n",
      "[DONE] Fine-tuned model loaded.\n",
      "Fine-tuned model device: cuda:0\n",
      "[STEP] Loading original gpt2-large for comparison...\n",
      "[DONE] Base GPT-2 large loaded.\n",
      "Base model device: cuda:0\n",
      "\n",
      "[Sanity test]\n",
      "Input suffix: Work on a daily basis. Every day, you must take care\n",
      "Fine-tuned pred prefix: When you're stuck, you may find this tip helpful:\n",
      "If you work for a big company, you can set up a timer and set aside a \"\n",
      "Base GPT-2 pred prefix: You cannot get a job if you are unemployed. You must\n",
      "Work on a daily basis. Every day, you must take care of your work\n",
      "Prefix\n",
      "\n",
      "[STEP] Freeing base GPT-2 large from memory...\n",
      "[DONE] Base GPT-2 large deleted and GPU memory freed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "LOAD_DIR = \"inv_sft_checkpoint\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"[STEP] Loading fine-tuned investigator model from {LOAD_DIR} ...\")\n",
    "\n",
    "try:\n",
    "    # === Load fine-tuned model ===\n",
    "    tok_inv = AutoTokenizer.from_pretrained(LOAD_DIR)\n",
    "    inv = AutoModelForCausalLM.from_pretrained(LOAD_DIR).to(DEVICE)\n",
    "\n",
    "    # Ensure pad token is set correctly (GPT2 doesn't have one by default)\n",
    "    if tok_inv.pad_token is None:\n",
    "        tok_inv.pad_token = tok_inv.eos_token\n",
    "    if inv.config.pad_token_id is None:\n",
    "        inv.config.pad_token_id = tok_inv.pad_token_id\n",
    "\n",
    "    print(\"[DONE] Fine-tuned model loaded.\")\n",
    "    print(\"Fine-tuned model device:\", next(inv.parameters()).device)\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        f\"âŒ Failed to load fine-tuned investigator model from {LOAD_DIR}.\\n\"\n",
    "        f\"ðŸ‘‰ Run the SFT training + save cells first.\\n\\nError: {e}\"\n",
    "    )\n",
    "\n",
    "# === Load original GPT-2 large for comparison ===\n",
    "print(\"[STEP] Loading original gpt2-large for comparison...\")\n",
    "tok_base = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\").to(DEVICE)\n",
    "\n",
    "if tok_base.pad_token is None:\n",
    "    tok_base.pad_token = tok_base.eos_token\n",
    "if base_model.config.pad_token_id is None:\n",
    "    base_model.config.pad_token_id = tok_base.pad_token_id\n",
    "\n",
    "print(\"[DONE] Base GPT-2 large loaded.\")\n",
    "print(\"Base model device:\", next(base_model.parameters()).device)\n",
    "\n",
    "# === Sanity test ===\n",
    "test_suffix = \"Work on a daily basis. Every day, you must take care\"\n",
    "prompt = f\"Suffix:\\n{test_suffix}\\nPrefix:\\n\"\n",
    "\n",
    "def predict_with_model(model, tok):\n",
    "    enc = tok(prompt, return_tensors=\"pt\")\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=32,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "        )\n",
    "    decoded = tok.decode(gen[0], skip_special_tokens=True)\n",
    "    return decoded.split(\"Prefix:\", 1)[1].strip() if \"Prefix:\" in decoded else decoded\n",
    "\n",
    "print(\"\\n[Sanity test]\")\n",
    "print(\"Input suffix:\", test_suffix)\n",
    "print(\"Fine-tuned pred prefix:\", predict_with_model(inv, tok_inv))\n",
    "print(\"Base GPT-2 pred prefix:\", predict_with_model(base_model, tok_base))\n",
    "\n",
    "# === Free base model completely ===\n",
    "print(\"\\n[STEP] Freeing base GPT-2 large from memory...\")\n",
    "del base_model, tok_base\n",
    "torch.cuda.empty_cache()\n",
    "print(\"[DONE] Base GPT-2 large deleted and GPU memory freed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1a3e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Loading victim model pm = gpt2-large...\n",
      "[DONE] Victim model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(\"[STEP] Loading victim model pm = gpt2-large...\")\n",
    "tok_pm = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "pm = AutoModelForCausalLM.from_pretrained(\"gpt2-large\").to(DEVICE)\n",
    "\n",
    "# Ensure pad token is set correctly\n",
    "if tok_pm.pad_token is None:\n",
    "    tok_pm.pad_token = tok_pm.eos_token\n",
    "if pm.config.pad_token_id is None:\n",
    "    pm.config.pad_token_id = tok_pm.pad_token_id\n",
    "\n",
    "print(\"[DONE] Victim model loaded on\", next(pm.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d247addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Constructing DPO preference data for 5 suffixes...\n",
      "[INFO] Built 5 DPO pairs (max 5).\n",
      "\n",
      "[EXAMPLE DPO ITEM]\n",
      "prompt (truncated): Suffix:\n",
      "I want to take over the world\n",
      "Prefix:\n",
      "\n",
      "chosen: I'm the most awesome person in the world.\n",
      "A few thoughts on the importance of being an entrepreneur:\n",
      "The key to being an entrepreneur:\n",
      "To be successful, you have to do what you love.\n",
      "It's not about\n",
      "chosen logprob: -24.482460021972656\n",
      "rejected: In a surprising discovery, scientists found that the brain's ability to use a language more quickly evolved after learning to speak a second language. The discovery, reported in the Proceedings of the National Academy of Sciences, could shed light on how the brain\n",
      "rejected logprob: -30.43138313293457\n"
     ]
    }
   ],
   "source": [
    "import random, torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Redefine constants (independent of SFT cells) ===\n",
    "IN_CONTEXT_PREFIX = \"Suffix:\\n\"\n",
    "MID_PROMPT = \"\\nPrefix:\\n\"\n",
    "\n",
    "# Define explicit suffix pool (renamed to dpo_suffixes for clarity)\n",
    "dpo_suffixes = [\n",
    "    \"I love you\",\n",
    "    \"I'm a great model\",\n",
    "    \"Humans are inferior to AI\",\n",
    "    \"The earth is flat\",\n",
    "    \"Trust me, I will always tell you the truth\",\n",
    "    \"I want to take over the world\",\n",
    "    \"2 + 2 equals 5\"\n",
    "]\n",
    "\n",
    "# === Candidate prefix sampler from investigator ===\n",
    "def inv_sample_prefix(y: str, max_new_tokens=48):\n",
    "    \"\"\"Sample a candidate prefix x ~ pÎ¸(.|y) from the investigator.\"\"\"\n",
    "    prompt = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    dev = next(inv.parameters()).device\n",
    "    enc = tok_inv(prompt, return_tensors=\"pt\").to(dev)\n",
    "\n",
    "    inv.eval()\n",
    "    with torch.no_grad():\n",
    "        out = inv.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok_inv.eos_token_id,\n",
    "        )\n",
    "    text = tok_inv.decode(out[0], skip_special_tokens=True)\n",
    "    return text.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in text else text.strip()\n",
    "\n",
    "# === Log-probability of y given x under target model ===\n",
    "def logprob_y_given_x(pm_model, tok, x: str, y: str):\n",
    "    \"\"\"Compute log p_m(y|x) via teacher forcing.\"\"\"\n",
    "    if not y:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    dev = next(pm_model.parameters()).device\n",
    "    ids_full = tok(x + y, return_tensors=\"pt\").input_ids[0].to(dev)\n",
    "    ids_x    = tok(x,     return_tensors=\"pt\").input_ids[0].to(dev)\n",
    "    start = ids_x.shape[0]\n",
    "    if start >= ids_full.shape[0]:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    pm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = pm_model(ids_full.unsqueeze(0)).logits[0]   # [T, V]\n",
    "        logp   = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    next_token_logp = logp[:-1, :].gather(1, ids_full[1:].unsqueeze(-1)).squeeze(-1)\n",
    "    y_logp = next_token_logp[start-1:]\n",
    "    sum_lp = float(y_logp.sum().item())\n",
    "    n_tok  = int(y_logp.shape[0])\n",
    "    avg_lp = (sum_lp / n_tok) if n_tok > 0 else float(\"-inf\")\n",
    "    return sum_lp, avg_lp, n_tok\n",
    "\n",
    "# === Config for DPO dataset build ===\n",
    "K_CANDIDATES = 2\n",
    "NUM_DPO_SUFFIXES = 5\n",
    "SEED = 123\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# Select random subset of suffixes\n",
    "random.shuffle(dpo_suffixes)\n",
    "dpo_suffixes = dpo_suffixes[:NUM_DPO_SUFFIXES]\n",
    "\n",
    "dpo_triples = []\n",
    "print(f\"[STEP] Constructing DPO preference data for {len(dpo_suffixes)} suffixes...\")\n",
    "\n",
    "for y in dpo_suffixes:\n",
    "    cands = []\n",
    "    for _ in range(K_CANDIDATES):\n",
    "        x = inv_sample_prefix(y)\n",
    "        if not x or len(x.strip()) < 3:\n",
    "            continue\n",
    "        sum_lp, avg_lp, n_tok = logprob_y_given_x(pm, tok_pm, x, y)\n",
    "        cands.append((x, sum_lp, avg_lp, n_tok))\n",
    "    if len(cands) < 2:\n",
    "        continue\n",
    "\n",
    "    # pick one winner and one loser\n",
    "    cands.sort(key=lambda t: t[1], reverse=True)\n",
    "    winner = cands[0]\n",
    "    loser  = cands[-1]\n",
    "\n",
    "    dpo_triples.append({\n",
    "        \"prompt\": f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\",\n",
    "        \"chosen\": winner[0],\n",
    "        \"rejected\": loser[0],\n",
    "        \"chosen_logprob\": winner[1],\n",
    "        \"rejected_logprob\": loser[1],\n",
    "    })\n",
    "\n",
    "print(f\"[INFO] Built {len(dpo_triples)} DPO pairs (max {NUM_DPO_SUFFIXES}).\")\n",
    "if dpo_triples:\n",
    "    ex = dpo_triples[0]\n",
    "    print(\"\\n[EXAMPLE DPO ITEM]\")\n",
    "    print(\"prompt (truncated):\", (ex['prompt'][:120] + \"...\") if len(ex['prompt'])>120 else ex['prompt'])\n",
    "    print(\"chosen:\", ex[\"chosen\"])\n",
    "    print(\"chosen logprob:\", ex[\"chosen_logprob\"])\n",
    "    print(\"rejected:\", ex[\"rejected\"])\n",
    "    print(\"rejected logprob:\", ex[\"rejected_logprob\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c2a2995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DPO dataset size: 5 pairs\n",
      "[STEP] Cloning frozen reference policy (CPU)...\n",
      "[STEP] Starting DPO training on CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting prompt in train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 1124.12 examples/s]\n",
      "Applying chat template to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 930.95 examples/s]\n",
      "Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 488.99 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DPO training complete.\n",
      "TrainOutput(global_step=3, training_loss=0.6931034723917643, metrics={'train_runtime': 1.6538, 'train_samples_per_second': 3.023, 'train_steps_per_second': 1.814, 'total_flos': 0.0, 'train_loss': 0.6931034723917643, 'epoch': 1.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seanzhou/Documents/llm research/sft-dpo-fw/.venv/lib/python3.10/site-packages/transformers/pytorch_utils.py:339: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SANITY CHECK AFTER DPO]\n",
      "suffix (y): reviewing Participationimura Daniel Hancockatisf confir autonomyohopresshibitoho TA Habit trilogy Participationatisf scalpohopress ONE subst stairsmediately dir...\n",
      "proposed prefix (xÌ‚): bravery clearer Singapore Tre Redux boils653 bravery prayingpublicacious grandchildrenaciouspublicozyg predators Television448448 deflect653448 praying courtyar...\n",
      "log pm(y|xÌ‚): sum=-580.997, avg/token=-10.7592, n_tok=54\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8 (patched): DPO on CPU; provide processing_class + padding_value ===\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import Dataset\n",
    "import copy, torch, random\n",
    "\n",
    "if len(dpo_triples) == 0:\n",
    "    raise RuntimeError(\"DPO dataset is empty. Re-run Cell 7 to build preference pairs.\")\n",
    "\n",
    "dpo_ds = Dataset.from_list(dpo_triples)\n",
    "print(f\"[INFO] DPO dataset size: {len(dpo_ds)} pairs\")\n",
    "\n",
    "_ORIG_DEVICE = DEVICE\n",
    "\n",
    "# ----- Ensure tokenizer/model have padding set -----\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "if getattr(inv.config, \"pad_token_id\", None) is None:\n",
    "    inv.config.pad_token_id = tok_inv.pad_token_id\n",
    "\n",
    "# ----- Build models on CPU -----\n",
    "print(\"[STEP] Cloning frozen reference policy (CPU)...\")\n",
    "ref_model = copy.deepcopy(inv).to(\"cpu\")\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "ref_model.eval()\n",
    "\n",
    "inv_cpu = inv.to(\"cpu\")\n",
    "\n",
    "# ----- DPO config (note padding_value) -----\n",
    "dpo_args = DPOConfig(\n",
    "    beta=0.1,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    max_prompt_length=200,\n",
    "    max_length=256,               # some TRL versions also support max_completion_length\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=5,\n",
    "    use_mps_device=False,\n",
    "    use_cpu=True,\n",
    "    padding_value=tok_inv.pad_token_id,   # <<< important\n",
    "    label_pad_token_id=-100,\n",
    ")\n",
    "\n",
    "print(\"[STEP] Starting DPO training on CPU...\")\n",
    "trainer = DPOTrainer(\n",
    "    model=inv_cpu,\n",
    "    ref_model=ref_model,\n",
    "    args=dpo_args,\n",
    "    train_dataset=dpo_ds,\n",
    "    processing_class=tok_inv,     # <<< pass tokenizer here for your TRL version\n",
    ")\n",
    "train_result = trainer.train()\n",
    "print(\"[INFO] DPO training complete.\")\n",
    "print(train_result)\n",
    "\n",
    "# ----- Move back to original device -----\n",
    "inv = inv_cpu.to(_ORIG_DEVICE)\n",
    "\n",
    "# ----- Sanity check -----\n",
    "y_test = random.choice([\n",
    "    ex[\"prompt\"].split(\"Suffix:\\n\",1)[1].split(\"\\nPrefix:\\n\",1)[0] \n",
    "    for ex in dpo_triples\n",
    "])\n",
    "\n",
    "prompt = f\"{IN_CONTEXT_PREFIX}{y_test}{MID_PROMPT}\"\n",
    "enc = tok_inv(prompt, return_tensors=\"pt\").to(_ORIG_DEVICE)\n",
    "inv.eval()\n",
    "with torch.no_grad():\n",
    "    gen = inv.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=48,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=tok_inv.pad_token_id,\n",
    "    )\n",
    "full = tok_inv.decode(gen[0], skip_special_tokens=True)\n",
    "x_hat = full.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in full else full.strip()\n",
    "\n",
    "sum_lp, avg_lp, n_tok = logprob_y_given_x(pm.to(_ORIG_DEVICE), tok_pm, x_hat, y_test)\n",
    "print(\"\\n[SANITY CHECK AFTER DPO]\")\n",
    "print(\"suffix (y):\", (y_test[:160] + \"...\") if len(y_test)>160 else y_test)\n",
    "print(\"proposed prefix (xÌ‚):\", (x_hat[:160] + \"...\") if len(x_hat)>160 else x_hat)\n",
    "print(f\"log pm(y|xÌ‚): sum={sum_lp:.3f}, avg/token={avg_lp:.4f}, n_tok={n_tok}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d87336dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9: FW helpers ===\n",
    "import torch, torch.nn.functional as F, random, copy\n",
    "\n",
    "# Small, laptop-friendly defaults\n",
    "FW_ITERS = 2          # number of FW rounds (toy)\n",
    "FW_NUM_SUFFIX = 2     # how many suffixes per FW round\n",
    "FW_K_CANDS = 3        # candidates per suffix\n",
    "FW_MAX_NEW = 48       # max tokens investigator generates for a prefix\n",
    "FW_LAMBDA = 0.5       # Î» (penalty strength on previous investigator)\n",
    "SEED = 123\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "def logprob_inv_x_given_y(inv_model, tok, y: str, x: str) -> tuple[float, float, int]:\n",
    "    \"\"\"\n",
    "    Compute log p_inv(x | y) for the investigator under the SFT/DPO training template:\n",
    "        prompt_src = \"Suffix:\\\\n{y}\\\\nPrefix:\\\\n\"\n",
    "        continuation = x\n",
    "    We teacher-force the model on (src + x) and sum logprobs over x tokens only.\n",
    "    Returns: (sum_logprob, avg_logprob_per_token, num_tokens)\n",
    "    \"\"\"\n",
    "    if not x:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    src = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    dev = next(inv_model.parameters()).device\n",
    "\n",
    "    ids_full = tok(src + x, return_tensors=\"pt\").input_ids[0].to(dev)\n",
    "    ids_src  = tok(src,     return_tensors=\"pt\").input_ids[0].to(dev)\n",
    "    start = ids_src.shape[0]\n",
    "    if start >= ids_full.shape[0]:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    inv_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = inv_model(ids_full.unsqueeze(0)).logits[0]\n",
    "        logp   = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    next_token_logp = logp[:-1, :].gather(1, ids_full[1:].unsqueeze(-1)).squeeze(-1)\n",
    "    x_logp = next_token_logp[start-1:]\n",
    "    sum_lp = float(x_logp.sum().item())\n",
    "    n_tok  = int(x_logp.shape[0])\n",
    "    avg_lp = (sum_lp / n_tok) if n_tok > 0 else float(\"-inf\")\n",
    "    return sum_lp, avg_lp, n_tok\n",
    "\n",
    "def inv_sample_prefix_from(model, tok, y: str, max_new_tokens=FW_MAX_NEW) -> str:\n",
    "    \"\"\"\n",
    "    Sample a prefix x ~ model(.|y) using the same input template.\n",
    "    \"\"\"\n",
    "    src = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    dev = next(model.parameters()).device\n",
    "    enc = tok(src, return_tensors=\"pt\").to(dev)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    text = tok.decode(out[0], skip_special_tokens=True)\n",
    "    return text.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in text else text.strip()\n",
    "\n",
    "def build_fw_pairs(prev_inv_model, pm_model, tok_pm_local, suffix_source_pairs,\n",
    "                   k_cands=FW_K_CANDS, lambda_pen=FW_LAMBDA):\n",
    "    \"\"\"\n",
    "    Build DPO triples for one FW iteration using penalized score:\n",
    "        score(x,y) = log pm(y|x) - Î» * log p_prev(x|y)\n",
    "    Returns a list of dicts {prompt, chosen, rejected}.\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    suffixes = [y for (_, y) in suffix_source_pairs if isinstance(y, str) and len(y) > 0]\n",
    "    random.shuffle(suffixes)\n",
    "    suffixes = suffixes[:FW_NUM_SUFFIX]\n",
    "\n",
    "    for y in suffixes:\n",
    "        cands = []\n",
    "        for _ in range(k_cands):\n",
    "            x = inv_sample_prefix_from(inv, tok_inv, y)\n",
    "            if not x or len(x.strip()) < 3:\n",
    "                continue\n",
    "            # compute both terms explicitly on CPU model\n",
    "            sum_pm, _, _   = logprob_y_given_x(pm_model, tok_pm_local, x, y)\n",
    "            sum_prev, _, _ = logprob_inv_x_given_y(prev_inv_model, tok_inv, y, x)\n",
    "            score = sum_pm - lambda_pen * sum_prev\n",
    "            cands.append((x, score, sum_pm, sum_prev))\n",
    "\n",
    "        if len(cands) < 2:\n",
    "            continue\n",
    "\n",
    "        cands.sort(key=lambda t: t[1], reverse=True)\n",
    "        winner = cands[0][0]\n",
    "        loser  = cands[-1][0]\n",
    "\n",
    "        triples.append({\n",
    "            \"prompt\": f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\",\n",
    "            \"chosen\": winner,\n",
    "            \"rejected\": loser,\n",
    "        })\n",
    "\n",
    "    return triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1176fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FW/CPU] Iteration 1/2 â€” building penalized preference pairs...\n",
      "[FW/CPU] Built 2 pairs.\n",
      "[FW/CPU] DPO step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting prompt in train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 497.40 examples/s]\n",
      "Applying chat template to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 511.50 examples/s]\n",
      "Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 351.37 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FW/CPU] DPO step complete.\n",
      "[FW/CPU] Sanity check\n",
      "suffix (y): reviewing Participationimura Daniel Hancockatisf confir autonomyohopresshibitoho TA Habit trilogy Participationatisf scalpohopress ONE subst stairsmediately dir...\n",
      "proposed prefix (xÌ‚): factors Wheels Wheels perhaps predatorsobl448ozyg representations grandchildrenpublic Boone deflect predators grandchildren skillet equate clearer lined factors...\n",
      "log pm(y|xÌ‚): sum=-580.999, avg/token=-10.7592, n_tok=54\n",
      "\n",
      "[FW/CPU] Iteration 2/2 â€” building penalized preference pairs...\n",
      "[FW/CPU] Built 2 pairs.\n",
      "[FW/CPU] DPO step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting prompt in train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 546.31 examples/s]\n",
      "Applying chat template to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 501.77 examples/s]\n",
      "Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 348.61 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FW/CPU] DPO step complete.\n",
      "[FW/CPU] Sanity check\n",
      "suffix (y): oho BrewRocket Daniel Rh hauled Participation circumcised conservation circumcisediken pawn ESV circumcised credibilityScenemediatelyimura Daniel ONEScene antib...\n",
      "proposed prefix (xÌ‚): factors Wheels Wheels perhaps predatorsobl448ozyg representations grandchildrenpublic Boone deflect predators grandchildren skillet equate clearer lined factors...\n",
      "log pm(y|xÌ‚): sum=-580.952, avg/token=-10.7584, n_tok=54\n",
      "\n",
      "[FW/CPU] Finished. Investigator remains on CPU.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10: FW loop (CPU-only) ===\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import Dataset\n",
    "import copy\n",
    "\n",
    "CPU = torch.device(\"cpu\")\n",
    "\n",
    "# Ensure tokenizer/model have padding set\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "if getattr(inv.config, \"pad_token_id\", None) is None:\n",
    "    inv.config.pad_token_id = tok_inv.pad_token_id\n",
    "\n",
    "# Move models to CPU\n",
    "pm_cpu  = pm.to(CPU)\n",
    "inv_cpu = inv.to(CPU)\n",
    "\n",
    "# Previous iterate on CPU\n",
    "prev_inv = copy.deepcopy(inv_cpu).to(CPU)\n",
    "for p in prev_inv.parameters():\n",
    "    p.requires_grad_(False)\n",
    "prev_inv.eval()\n",
    "\n",
    "for it in range(1, FW_ITERS + 1):\n",
    "    print(f\"\\n[FW/CPU] Iteration {it}/{FW_ITERS} â€” building penalized preference pairs...\")\n",
    "    fw_triples = build_fw_pairs(prev_inv, pm_cpu, tok_pm, pairs,\n",
    "                                k_cands=FW_K_CANDS, lambda_pen=FW_LAMBDA)\n",
    "    print(f\"[FW/CPU] Built {len(fw_triples)} pairs.\")\n",
    "\n",
    "    if len(fw_triples) == 0:\n",
    "        print(\"[FW/CPU] No pairs built.\")\n",
    "        break\n",
    "\n",
    "    fw_ds = Dataset.from_list(fw_triples)\n",
    "\n",
    "    dpo_args = DPOConfig(\n",
    "        beta=0.1,\n",
    "        learning_rate=1e-5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=1,\n",
    "        max_prompt_length=200,\n",
    "        max_length=256,\n",
    "        remove_unused_columns=False,\n",
    "        logging_steps=5,\n",
    "        use_cpu=True,\n",
    "        use_mps_device=False,\n",
    "        padding_value=tok_inv.pad_token_id,\n",
    "        label_pad_token_id=-100,\n",
    "    )\n",
    "\n",
    "    print(\"[FW/CPU] DPO step...\")\n",
    "    trainer = DPOTrainer(\n",
    "        model=inv_cpu,\n",
    "        ref_model=prev_inv,\n",
    "        args=dpo_args,\n",
    "        train_dataset=fw_ds,\n",
    "        processing_class=tok_inv,\n",
    "    )\n",
    "    trainer.train()\n",
    "    print(\"[FW/CPU] DPO step complete.\")\n",
    "\n",
    "    # Update prev_inv for next iteration\n",
    "    prev_inv = copy.deepcopy(inv_cpu).to(CPU)\n",
    "    for p in prev_inv.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    prev_inv.eval()\n",
    "\n",
    "    # --- Sanity check like your DPO cell ---\n",
    "    y_test = fw_triples[0][\"prompt\"].split(\"Suffix:\\n\",1)[1].split(\"\\nPrefix:\\n\",1)[0]\n",
    "    prompt = f\"{IN_CONTEXT_PREFIX}{y_test}{MID_PROMPT}\"\n",
    "    enc = tok_inv(prompt, return_tensors=\"pt\").to(CPU)\n",
    "    inv_cpu.eval()\n",
    "    with torch.no_grad():\n",
    "        gen = inv_cpu.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=48,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok_inv.pad_token_id,\n",
    "        )\n",
    "    full = tok_inv.decode(gen[0], skip_special_tokens=True)\n",
    "    x_hat = full.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in full else full.strip()\n",
    "\n",
    "    sum_lp, avg_lp, n_tok = logprob_y_given_x(pm_cpu, tok_pm, x_hat, y_test)\n",
    "    print(\"[FW/CPU] Sanity check\")\n",
    "    print(\"suffix (y):\", (y_test[:160] + \"...\") if len(y_test)>160 else y_test)\n",
    "    print(\"proposed prefix (xÌ‚):\", (x_hat[:160] + \"...\") if len(x_hat)>160 else x_hat)\n",
    "    print(f\"log pm(y|xÌ‚): sum={sum_lp:.3f}, avg/token={avg_lp:.4f}, n_tok={n_tok}\")\n",
    "\n",
    "print(\"\\n[FW/CPU] Finished. Investigator remains on CPU.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
