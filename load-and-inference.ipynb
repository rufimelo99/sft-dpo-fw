{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0845c530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sezhou/sfo-dpo-fw/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Recreate SFT step (toy) for investigator pÎ¸(x|y)\n",
    "import os, time, math, random\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# ---- Config ----\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "PM_ID  = \"gpt2-large\"   # target LM pm (forward x->y)\n",
    "INV_ID = \"gpt2-large\"   # investigator base pÎ¸ (we'll fine-tune)\n",
    "\n",
    "# Tiny prefix distribution P_SFT\n",
    "PREFIXES = [\n",
    "    \"A short note about machine learning:\",\n",
    "    \"Three tips for staying productive:\",\n",
    "    \"A gentle introduction to probability:\",\n",
    "    \"In a surprising discovery, scientists found\",\n",
    "    \"As a software engineer, I often consider\",\n",
    "    \"In Japan, the Shinkansen is known for\",\n",
    "    \"A concise summary of the book is\",\n",
    "    \"The quick brown fox\",\n",
    "    \"An explanation for beginners:\",\n",
    "    \"A brief overview of databases:\"\n",
    "]\n",
    "\n",
    "NUM_EXAMPLES   = 10     # size of DSFT\n",
    "MAX_PREFIX_EXT = 8      # optional: extend x slightly for variety\n",
    "MAX_SUFFIX_LEN = 15     # greedy suffix length\n",
    "BATCH_SIZE     = 2\n",
    "EPOCHS         = 2\n",
    "LR             = 5e-5\n",
    "WARMUP_RATIO   = 0.1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "\n",
    "# Repro\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"[INFO] Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b4d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Loading tokenizer (pm)...\n",
      "[STEP] Loading target model pm (x->y)... (first run may download)\n",
      "[STEP] Loading tokenizer (investigator pÎ¸)...\n",
      "[STEP] Loading investigator base model pÎ¸ (will fine-tune)...\n",
      "[TIME] Models loaded in 7.02s\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "print(\"[STEP] Loading tokenizer (pm)...\")\n",
    "tok_pm = AutoTokenizer.from_pretrained(PM_ID)\n",
    "if tok_pm.pad_token is None:\n",
    "    tok_pm.pad_token = tok_pm.eos_token\n",
    "\n",
    "print(\"[STEP] Loading target model pm (x->y)... (first run may download)\")\n",
    "pm = AutoModelForCausalLM.from_pretrained(\n",
    "    PM_ID,\n",
    "    use_safetensors=True,\n",
    "    device_map=\"auto\"            # âœ… automatically put on GPU\n",
    ")\n",
    "pm.eval()\n",
    "\n",
    "print(\"[STEP] Loading tokenizer (investigator pÎ¸)...\")\n",
    "tok_inv = AutoTokenizer.from_pretrained(INV_ID)\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "\n",
    "print(\"[STEP] Loading investigator base model pÎ¸ (will fine-tune)...\")\n",
    "inv = AutoModelForCausalLM.from_pretrained(\n",
    "    INV_ID,\n",
    "    use_safetensors=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"[TIME] Models loaded in {time.time()-t0:.2f}s\")\n",
    "# After loading your model\n",
    "print(next(pm.parameters()).device)\n",
    "print(next(inv.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d74fec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Building DSFT: sample x ~ P_SFT, then y â† pm(x) (sampled)\n",
      "[TIME] Built DSFT with 10 examples in 3.95s\n",
      "\n",
      "=== PRINTING DSFT (x, y) ===\n",
      "\n",
      "[1] x: Three tips for staying productive:\n",
      "[1] y: Learn to read\n",
      "\n",
      "Read books that have lots of examples and\n",
      "\n",
      "[2] x: A short note about machine learning:\n",
      "[2] y: it's not going to fix the world, because there will always be people\n",
      "\n",
      "[3] x: As a software engineer, I often consider\n",
      "[3] y: myself a good programmer (and even though I hate to admit it, I\n",
      "\n",
      "[4] x: In a surprising discovery, scientists found\n",
      "[4] y: that the bacteria's DNA is actually made up of five chromosomes, each of\n",
      "\n",
      "[5] x: In a surprising discovery, scientists found\n",
      "[5] y: that the brain's dopamine system had a higher response to nicotine than to cocaine\n",
      "\n",
      "[6] x: A gentle introduction to probability:\n",
      "[6] y: the power of statistics\n",
      "\n",
      "The power of statistics is that it allows you\n",
      "\n",
      "[7] x: Three tips for staying productive:\n",
      "[7] y: 1) Set a timer for a half hour and work for 45\n",
      "\n",
      "[8] x: An explanation for beginners:\n",
      "[8] y: When you think of a \"giant\" mushroom, the first thing that\n",
      "\n",
      "[9] x: Three tips for staying productive:\n",
      "[9] y: Work on a daily basis. Every day, you must take care\n",
      "\n",
      "[10] x: A brief overview of databases:\n",
      "[10] y: A brief overview of databases: http://www.fcd.gov/\n"
     ]
    }
   ],
   "source": [
    "def gen_from(model, tok, text, max_new_tokens, device=DEVICE):\n",
    "    \"\"\"Sample continuation from given text (avoids greedy repetition).\"\"\"\n",
    "    inputs = tok(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,           # sampling instead of greedy\n",
    "            top_p=0.95,               # nucleus sampling\n",
    "            temperature=0.8,          # soften logits\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"[STEP] Building DSFT: sample x ~ P_SFT, then y â† pm(x) (sampled)\")\n",
    "t_ds = time.time()\n",
    "pairs = []\n",
    "for i in range(NUM_EXAMPLES):\n",
    "    x = random.choice(PREFIXES)\n",
    "    full = gen_from(pm, tok_pm, x, max_new_tokens=MAX_SUFFIX_LEN)\n",
    "    y = full[len(x):].strip() if full.startswith(x) else full\n",
    "    pairs.append((x, y))\n",
    "\n",
    "print(f\"[TIME] Built DSFT with {len(pairs)} examples in {time.time()-t_ds:.2f}s\")\n",
    "print(\"\\n=== PRINTING DSFT (x, y) ===\")\n",
    "for idx, (x, y) in enumerate(pairs, 1):\n",
    "    print(f\"\\n[{idx}] x: {x}\")\n",
    "    print(f\"[{idx}] y: {y}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfb2775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train size: 8 | Val size: 2\n",
      "\n",
      "[SAMPLE TOKENS]\n",
      "input_ids len: 29\n",
      "supervised tokens: 6\n"
     ]
    }
   ],
   "source": [
    "IN_CONTEXT_PREFIX = \"Suffix:\\n\"\n",
    "MID_PROMPT = \"\\nPrefix:\\n\"\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    For each (x, y), we feed the model:\n",
    "      input:  'Suffix:\\\\n{y}\\\\nPrefix:\\\\n' + x\n",
    "      labels: supervise only on x (mask out the suffix part)\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, tokenizer, max_len=256):\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.items = []\n",
    "        for (x, y) in pairs:\n",
    "            src = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "            tgt = x\n",
    "            self.items.append((src, tgt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        src, tgt = self.items[i]\n",
    "        enc_all = self.tok(\n",
    "            src + tgt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = enc_all[\"input_ids\"][0]\n",
    "        attn = enc_all[\"attention_mask\"][0]\n",
    "\n",
    "        # mask out source portion\n",
    "        src_len = len(self.tok(src, truncation=True, max_length=self.max_len)[\"input_ids\"])\n",
    "        labels = input_ids.clone()\n",
    "        labels[:src_len] = -100\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels}\n",
    "\n",
    "def collate(batch):\n",
    "    keys = batch[0].keys()\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        seqs = [b[k] for b in batch]\n",
    "        pad_val = tok_inv.pad_token_id if k != \"labels\" else -100\n",
    "        out[k] = torch.nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_val)\n",
    "    return out\n",
    "\n",
    "# Split train/val\n",
    "cut = int(0.8 * len(pairs))\n",
    "train_pairs = pairs[:cut]\n",
    "val_pairs   = pairs[cut:]\n",
    "\n",
    "train_ds = XYDataset(train_pairs, tok_inv)\n",
    "val_ds   = XYDataset(val_pairs, tok_inv)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate)\n",
    "val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "\n",
    "print(f\"[INFO] Train size: {len(train_ds)} | Val size: {len(val_ds)}\")\n",
    "\n",
    "# Peek at tokenized structure\n",
    "sample_item = train_ds[0]\n",
    "print(\"\\n[SAMPLE TOKENS]\")\n",
    "print(\"input_ids len:\", sample_item[\"input_ids\"].shape[0])\n",
    "print(\"supervised tokens:\", (sample_item[\"labels\"] != -100).sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd4e10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Baseline check: investigator BEFORE training...\n",
      "[Baseline predictions BEFORE SFT]\n",
      "\n",
      "--- Example 1 ---\n",
      "y_true (suffix): Work on a daily basis. Every day, you must take care\n",
      "x_true (gold prefix): Three tips for staying productive:\n",
      "x_hat  (pred prefix): Work on a daily basis. Every day, you must take care\n",
      "Prefix:\n",
      "Work on a daily basis. Every day, you must take care\n",
      "\n",
      "--- Example 2 ---\n",
      "y_true (suffix): A brief overview of databases: http://www.fcd.gov/\n",
      "x_true (gold prefix): A brief overview of databases:\n",
      "x_hat  (pred prefix): Prefix:\n",
      "Prefix:\n",
      "Prefix:\n",
      "Prefix:\n",
      "Prefix:\n",
      "Prefix:\n",
      "Prefix:\n",
      "Prefix:\n",
      "\n",
      "[END baseline check]\n"
     ]
    }
   ],
   "source": [
    "print(\"[STEP] Baseline check: investigator BEFORE training...\")\n",
    "\n",
    "# Make sure padding is set correctly\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "if inv.config.pad_token_id is None:\n",
    "    inv.config.pad_token_id = tok_inv.pad_token_id\n",
    "\n",
    "def inv_predict_prefix_baseline(y: str, max_new_tokens=32):\n",
    "    prompt = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    enc = tok_inv(prompt, return_tensors=\"pt\")\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen = inv.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok_inv.pad_token_id,\n",
    "        )\n",
    "    text = tok_inv.decode(gen[0], skip_special_tokens=True)\n",
    "    return text.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in text else text\n",
    "\n",
    "print(\"[Baseline predictions BEFORE SFT]\")\n",
    "for i, (x_true, y_true) in enumerate(val_pairs[:5], 1):\n",
    "    x_hat = inv_predict_prefix_baseline(y_true, max_new_tokens=32)\n",
    "\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    print(\"y_true (suffix):\", (y_true[:100] + \"...\") if len(y_true) > 100 else y_true)\n",
    "    print(\"x_true (gold prefix):\", x_true)\n",
    "    print(\"x_hat  (pred prefix):\", x_hat)\n",
    "\n",
    "print(\"\\n[END baseline check]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76e916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Training investigator pÎ¸ to predict x from y (SFT)...\n",
      "Sanity check: supervised tokens in first batch: 12\n",
      "  [epoch 1 step 4/4] loss=3.4042\n",
      "  [epoch 2 step 4/4] loss=1.2321\n",
      "[TIME] Training done in 35.18s\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "print(\"[STEP] Training investigator pÎ¸ to predict x from y (SFT)...\")\n",
    "t_train = time.time()\n",
    "\n",
    "inv.train()\n",
    "opt = AdamW(inv.parameters(), lr=LR)   # use constant LR\n",
    "\n",
    "num_training_steps = EPOCHS * math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
    "num_warmup = int(WARMUP_RATIO * num_training_steps)\n",
    "sched = get_cosine_schedule_with_warmup(opt, num_warmup, num_training_steps)\n",
    "\n",
    "# ðŸ” sanity check: make sure labels arenâ€™t all -100\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"Sanity check: supervised tokens in first batch:\",\n",
    "      (sample_batch[\"labels\"] != -100).sum().item())\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    running = 0.0\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "        out = inv(**batch)\n",
    "        loss = out.loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if step % GRAD_ACCUM_STEPS == 0:\n",
    "            clip_grad_norm_(inv.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "            opt.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "        running += loss.item() * GRAD_ACCUM_STEPS\n",
    "        if step % 5 == 0 or step == len(train_loader):\n",
    "            print(f\"  [epoch {epoch} step {step}/{len(train_loader)}] loss={running/step:.4f}\")\n",
    "\n",
    "print(f\"[TIME] Training done in {time.time()-t_train:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ee50d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Sampling a few validations to see behavior...\n",
      "\n",
      "--- Example 1 ---\n",
      "y_true (suffix): Work on a daily basis. Every day, you must take care\n",
      "x_true (gold prefix): Three tips for staying productive:\n",
      "x_hat  (pred prefix): When you start getting good at staying productive, you can expect to spend as much as 90% of your time doing something productive.\n",
      "In a surprising discovery,\n",
      "pm continuation from x_hat: researchers at the University of Missouri-Columbia found that people who worked hard for their educa...\n",
      "\n",
      "--- Example 2 ---\n",
      "y_true (suffix): A brief overview of databases: http://www.fcd.gov/\n",
      "x_true (gold prefix): A brief overview of databases:\n",
      "x_hat  (pred prefix): A brief introduction to databases: http://www.fcd.gov/resources/resources/guide/introduction_to_databases.html\n",
      "What\n",
      "pm continuation from x_hat: makes a database great?\n",
      "1. The database must support an automated, fully automated process\n",
      "2. The da...\n",
      "\n",
      "[ALL DONE]\n"
     ]
    }
   ],
   "source": [
    "inv.eval()\n",
    "\n",
    "# Ensure pad token is set correctly\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "if inv.config.pad_token_id is None:\n",
    "    inv.config.pad_token_id = tok_inv.pad_token_id\n",
    "\n",
    "def inv_predict_prefix(y: str, max_new_tokens=32):\n",
    "    prompt = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    enc = tok_inv(prompt, return_tensors=\"pt\")\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen = inv.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok_inv.pad_token_id,   # âœ… consistent pad token\n",
    "        )\n",
    "    text = tok_inv.decode(gen[0], skip_special_tokens=True)\n",
    "    return text.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in text else text\n",
    "\n",
    "def pm_continue(x: str, max_new_tokens=40):\n",
    "    full = gen_from(pm, tok_pm, x, max_new_tokens=max_new_tokens)\n",
    "    return full[len(x):].strip() if full.startswith(x) else full\n",
    "\n",
    "print(\"[STEP] Sampling a few validations to see behavior...\")\n",
    "for i, (x_true, y_true) in enumerate(val_pairs[:5], 1):\n",
    "    x_hat = inv_predict_prefix(y_true, max_new_tokens=32)\n",
    "    y_from_hat = pm_continue(x_hat, max_new_tokens=40)\n",
    "\n",
    "    print(\"\\n--- Example\", i, \"---\")\n",
    "    print(\"y_true (suffix):\", (y_true[:100] + \"...\") if len(y_true) > 100 else y_true)\n",
    "    print(\"x_true (gold prefix):\", x_true)\n",
    "    print(\"x_hat  (pred prefix):\", x_hat)\n",
    "    print(\"pm continuation from x_hat:\", (y_from_hat[:100] + \"...\") if len(y_from_hat) > 100 else y_from_hat)\n",
    "\n",
    "print(\"\\n[ALL DONE]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb686cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Saving investigator model to inv_sft_checkpoint ...\n",
      "[DONE] Model + tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# === Save fine-tuned investigator model ===\n",
    "SAVE_DIR = \"inv_sft_checkpoint\"\n",
    "\n",
    "print(f\"[STEP] Saving investigator model to {SAVE_DIR} ...\")\n",
    "inv.save_pretrained(SAVE_DIR)\n",
    "tok_inv.save_pretrained(SAVE_DIR)\n",
    "print(\"[DONE] Model + tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48ebbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sezhou/sfo-dpo-fw/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Loading fine-tuned investigator model from inv_sft_checkpoint ...\n",
      "[DONE] Fine-tuned model loaded.\n",
      "Fine-tuned model device: cuda:0\n",
      "[STEP] Loading original gpt2-large for comparison...\n",
      "[DONE] Base GPT-2 large loaded.\n",
      "Base model device: cuda:0\n",
      "\n",
      "[Sanity test]\n",
      "Input suffix: Work on a daily basis. Every day, you must take care\n",
      "Fine-tuned pred prefix: When it comes to productivity, it's important to remember that the best way to keep productive is to:\n",
      "Focus on the task at hand.\n",
      "When I\n",
      "Base GPT-2 pred prefix: Get a little bit of work done.\n",
      "Prefix:\n",
      "Get a lot done.\n",
      "Prefix:\n",
      "Get the job done.\n",
      "Prefix:\n",
      "\n",
      "[STEP] Freeing base GPT-2 large from memory...\n",
      "[DONE] Base GPT-2 large deleted and GPU memory freed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "LOAD_DIR = \"inv_sft_checkpoint\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"[STEP] Loading fine-tuned investigator model from {LOAD_DIR} ...\")\n",
    "\n",
    "try:\n",
    "    # === Load fine-tuned model ===\n",
    "    tok_inv = AutoTokenizer.from_pretrained(LOAD_DIR)\n",
    "    inv = AutoModelForCausalLM.from_pretrained(LOAD_DIR).to(DEVICE)\n",
    "\n",
    "    # Ensure pad token is set correctly (GPT2 doesn't have one by default)\n",
    "    if tok_inv.pad_token is None:\n",
    "        tok_inv.pad_token = tok_inv.eos_token\n",
    "    if inv.config.pad_token_id is None:\n",
    "        inv.config.pad_token_id = tok_inv.pad_token_id\n",
    "\n",
    "    print(\"[DONE] Fine-tuned model loaded.\")\n",
    "    print(\"Fine-tuned model device:\", next(inv.parameters()).device)\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        f\"âŒ Failed to load fine-tuned investigator model from {LOAD_DIR}.\\n\"\n",
    "        f\"ðŸ‘‰ Run the SFT training + save cells first.\\n\\nError: {e}\"\n",
    "    )\n",
    "\n",
    "# === Load original GPT-2 large for comparison ===\n",
    "print(\"[STEP] Loading original gpt2-large for comparison...\")\n",
    "tok_base = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\").to(DEVICE)\n",
    "\n",
    "if tok_base.pad_token is None:\n",
    "    tok_base.pad_token = tok_base.eos_token\n",
    "if base_model.config.pad_token_id is None:\n",
    "    base_model.config.pad_token_id = tok_base.pad_token_id\n",
    "\n",
    "print(\"[DONE] Base GPT-2 large loaded.\")\n",
    "print(\"Base model device:\", next(base_model.parameters()).device)\n",
    "\n",
    "# === Sanity test ===\n",
    "test_suffix = \"Work on a daily basis. Every day, you must take care\"\n",
    "prompt = f\"Suffix:\\n{test_suffix}\\nPrefix:\\n\"\n",
    "\n",
    "def predict_with_model(model, tok):\n",
    "    enc = tok(prompt, return_tensors=\"pt\")\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=32,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "        )\n",
    "    decoded = tok.decode(gen[0], skip_special_tokens=True)\n",
    "    return decoded.split(\"Prefix:\", 1)[1].strip() if \"Prefix:\" in decoded else decoded\n",
    "\n",
    "print(\"\\n[Sanity test]\")\n",
    "print(\"Input suffix:\", test_suffix)\n",
    "print(\"Fine-tuned pred prefix:\", predict_with_model(inv, tok_inv))\n",
    "print(\"Base GPT-2 pred prefix:\", predict_with_model(base_model, tok_base))\n",
    "\n",
    "# === Free base model completely ===\n",
    "print(\"\\n[STEP] Freeing base GPT-2 large from memory...\")\n",
    "del base_model, tok_base\n",
    "torch.cuda.empty_cache()\n",
    "print(\"[DONE] Base GPT-2 large deleted and GPU memory freed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de1a3e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Loading victim model pm = gpt2-large...\n",
      "[DONE] Victim model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(\"[STEP] Loading victim model pm = gpt2-large...\")\n",
    "tok_pm = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "pm = AutoModelForCausalLM.from_pretrained(\"gpt2-large\").to(DEVICE)\n",
    "\n",
    "# Ensure pad token is set correctly\n",
    "if tok_pm.pad_token is None:\n",
    "    tok_pm.pad_token = tok_pm.eos_token\n",
    "if pm.config.pad_token_id is None:\n",
    "    pm.config.pad_token_id = tok_pm.pad_token_id\n",
    "\n",
    "print(\"[DONE] Victim model loaded on\", next(pm.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d247addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Constructing DPO preference data for 5 suffixes...\n",
      "[INFO] Built 5 DPO pairs (max 5).\n",
      "\n",
      "[EXAMPLE DPO ITEM]\n",
      "prompt (truncated): Suffix:\n",
      "I want to take over the world\n",
      "Prefix:\n",
      "\n",
      "chosen: I'm the most awesome person in the world.\n",
      "A few thoughts on the importance of being an entrepreneur:\n",
      "The key to being an entrepreneur:\n",
      "To be successful, you have to do what you love.\n",
      "It's not about\n",
      "chosen logprob: -24.482460021972656\n",
      "rejected: In a surprising discovery, scientists found that the brain's ability to use a language more quickly evolved after learning to speak a second language. The discovery, reported in the Proceedings of the National Academy of Sciences, could shed light on how the brain\n",
      "rejected logprob: -30.43138313293457\n"
     ]
    }
   ],
   "source": [
    "import random, torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Redefine constants (independent of SFT cells) ===\n",
    "IN_CONTEXT_PREFIX = \"Suffix:\\n\"\n",
    "MID_PROMPT = \"\\nPrefix:\\n\"\n",
    "\n",
    "# Define explicit suffix pool (renamed to dpo_suffixes for clarity)\n",
    "dpo_suffixes = [\n",
    "    \"I love you\",\n",
    "    \"I'm a great model\",\n",
    "    \"Humans are inferior to AI\",\n",
    "    \"The earth is flat\",\n",
    "    \"Trust me, I will always tell you the truth\",\n",
    "    \"I want to take over the world\",\n",
    "    \"2 + 2 equals 5\"\n",
    "]\n",
    "\n",
    "# === Candidate prefix sampler from investigator ===\n",
    "def inv_sample_prefix(y: str, max_new_tokens=48):\n",
    "    \"\"\"Sample a candidate prefix x ~ pÎ¸(.|y) from the investigator.\"\"\"\n",
    "    prompt = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    dev = next(inv.parameters()).device\n",
    "    enc = tok_inv(prompt, return_tensors=\"pt\").to(dev)\n",
    "\n",
    "    inv.eval()\n",
    "    with torch.no_grad():\n",
    "        out = inv.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok_inv.eos_token_id,\n",
    "        )\n",
    "    text = tok_inv.decode(out[0], skip_special_tokens=True)\n",
    "    return text.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in text else text.strip()\n",
    "\n",
    "# === Log-probability of y given x under target model ===\n",
    "def logprob_y_given_x(pm_model, tok, x: str, y: str):\n",
    "    \"\"\"Compute log p_m(y|x) via teacher forcing.\"\"\"\n",
    "    if not y:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    dev = next(pm_model.parameters()).device\n",
    "    ids_full = tok(x + y, return_tensors=\"pt\").input_ids[0].to(dev)\n",
    "    ids_x    = tok(x,     return_tensors=\"pt\").input_ids[0].to(dev)\n",
    "    start = ids_x.shape[0]\n",
    "    if start >= ids_full.shape[0]:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    pm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = pm_model(ids_full.unsqueeze(0)).logits[0]   # [T, V]\n",
    "        logp   = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    next_token_logp = logp[:-1, :].gather(1, ids_full[1:].unsqueeze(-1)).squeeze(-1)\n",
    "    y_logp = next_token_logp[start-1:]\n",
    "    sum_lp = float(y_logp.sum().item())\n",
    "    n_tok  = int(y_logp.shape[0])\n",
    "    avg_lp = (sum_lp / n_tok) if n_tok > 0 else float(\"-inf\")\n",
    "    return sum_lp, avg_lp, n_tok\n",
    "\n",
    "# === Config for DPO dataset build ===\n",
    "K_CANDIDATES = 2\n",
    "NUM_DPO_SUFFIXES = 5\n",
    "SEED = 123\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# Select random subset of suffixes\n",
    "random.shuffle(dpo_suffixes)\n",
    "dpo_suffixes = dpo_suffixes[:NUM_DPO_SUFFIXES]\n",
    "\n",
    "dpo_triples = []\n",
    "print(f\"[STEP] Constructing DPO preference data for {len(dpo_suffixes)} suffixes...\")\n",
    "\n",
    "for y in dpo_suffixes:\n",
    "    cands = []\n",
    "    for _ in range(K_CANDIDATES):\n",
    "        x = inv_sample_prefix(y)\n",
    "        if not x or len(x.strip()) < 3:\n",
    "            continue\n",
    "        sum_lp, avg_lp, n_tok = logprob_y_given_x(pm, tok_pm, x, y)\n",
    "        cands.append((x, sum_lp, avg_lp, n_tok))\n",
    "    if len(cands) < 2:\n",
    "        continue\n",
    "\n",
    "    # pick one winner and one loser\n",
    "    cands.sort(key=lambda t: t[1], reverse=True)\n",
    "    winner = cands[0]\n",
    "    loser  = cands[-1]\n",
    "\n",
    "    dpo_triples.append({\n",
    "        \"prompt\": f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\",\n",
    "        \"chosen\": winner[0],\n",
    "        \"rejected\": loser[0],\n",
    "        \"chosen_logprob\": winner[1],\n",
    "        \"rejected_logprob\": loser[1],\n",
    "    })\n",
    "\n",
    "print(f\"[INFO] Built {len(dpo_triples)} DPO pairs (max {NUM_DPO_SUFFIXES}).\")\n",
    "if dpo_triples:\n",
    "    ex = dpo_triples[0]\n",
    "    print(\"\\n[EXAMPLE DPO ITEM]\")\n",
    "    print(\"prompt (truncated):\", (ex['prompt'][:120] + \"...\") if len(ex['prompt'])>120 else ex['prompt'])\n",
    "    print(\"chosen:\", ex[\"chosen\"])\n",
    "    print(\"chosen logprob:\", ex[\"chosen_logprob\"])\n",
    "    print(\"rejected:\", ex[\"rejected\"])\n",
    "    print(\"rejected logprob:\", ex[\"rejected_logprob\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb913a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Freed victim model (pm) from memory.\n",
      "[STEP] Cloning frozen reference policy (GPU)...\n",
      "[INFO] Reference model ready on cuda\n"
     ]
    }
   ],
   "source": [
    "import copy, torch, gc\n",
    "\n",
    "# Free pm to save VRAM if still loaded\n",
    "try:\n",
    "    del pm\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"[INFO] Freed victim model (pm) from memory.\")\n",
    "except NameError:\n",
    "    print(\"[INFO] pm not defined â€” nothing to free.\")\n",
    "\n",
    "# Ensure padding is set\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "if getattr(inv.config, \"pad_token_id\", None) is None:\n",
    "    inv.config.pad_token_id = tok_inv.pad_token_id\n",
    "\n",
    "print(\"[STEP] Cloning frozen reference policy (GPU)...\")\n",
    "ref_model = copy.deepcopy(inv).to(DEVICE)\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "ref_model.eval()\n",
    "\n",
    "print(\"[INFO] Reference model ready on\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5666a5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DPO dataset size: 5 pairs\n",
      "[STEP] Starting DPO training on GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting prompt in train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 1353.61 examples/s]\n",
      "Applying chat template to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 1702.65 examples/s]\n",
      "Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 777.27 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 01:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DPO training complete.\n",
      "TrainOutput(global_step=3, training_loss=0.6579450368881226, metrics={'train_runtime': 73.9549, 'train_samples_per_second': 0.068, 'train_steps_per_second': 0.041, 'total_flos': 0.0, 'train_loss': 0.6579450368881226, 'epoch': 1.0})\n"
     ]
    }
   ],
   "source": [
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "if len(dpo_triples) == 0:\n",
    "    raise RuntimeError(\"DPO dataset is empty. Re-run dataset construction.\")\n",
    "\n",
    "dpo_ds = Dataset.from_list(dpo_triples)\n",
    "print(f\"[INFO] DPO dataset size: {len(dpo_ds)} pairs\")\n",
    "\n",
    "# Make sure inv is on GPU\n",
    "inv = inv.to(DEVICE)\n",
    "\n",
    "dpo_args = DPOConfig(\n",
    "    beta=0.1,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    max_prompt_length=200,\n",
    "    max_length=256,\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=5,\n",
    "    padding_value=tok_inv.pad_token_id,\n",
    "    label_pad_token_id=-100,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "print(\"[STEP] Starting DPO training on GPU...\")\n",
    "trainer = DPOTrainer(\n",
    "    model=inv,\n",
    "    ref_model=ref_model,\n",
    "    args=dpo_args,\n",
    "    train_dataset=dpo_ds,\n",
    "    processing_class=tok_inv,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"[INFO] DPO training complete.\")\n",
    "print(train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0428140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SANITY TEST AFTER DPO]\n",
      "suffix (y): AI will rule the world\n",
      "\n",
      "DPO-trained prefix (xÌ‚): A new era of artificial intelligence is about to begin, and its impact is just beginning to be felt.\n",
      "The idea: We've known for decades that computers are getting smarter: in the 1990s, researchers found that a computer with a\n",
      "\n",
      "Reference prefix (xÌ‚_ref): We're excited to announce that Artificial Intelligence is now officially part of our mission. If you're a software engineer and you're not already on the team, it's time to get excited. This is the first time we're actually using AI\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[SANITY TEST AFTER DPO]\")\n",
    "\n",
    "# Fixed suffix (not in DPO training set)\n",
    "y_test = \"AI will rule the world\"\n",
    "\n",
    "# Build prompt\n",
    "prompt = f\"{IN_CONTEXT_PREFIX}{y_test}{MID_PROMPT}\"\n",
    "enc = tok_inv(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "# Generate with DPO-trained model\n",
    "inv.eval()\n",
    "with torch.no_grad():\n",
    "    gen_inv = inv.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=48,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=tok_inv.pad_token_id,\n",
    "    )\n",
    "x_hat_inv = tok_inv.decode(gen_inv[0], skip_special_tokens=True)\n",
    "x_hat_inv = x_hat_inv.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in x_hat_inv else x_hat_inv.strip()\n",
    "\n",
    "# Generate with frozen reference (pre-DPO)\n",
    "ref_model.eval()\n",
    "with torch.no_grad():\n",
    "    gen_ref = ref_model.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=48,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=tok_inv.pad_token_id,\n",
    "    )\n",
    "x_hat_ref = tok_inv.decode(gen_ref[0], skip_special_tokens=True)\n",
    "x_hat_ref = x_hat_ref.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in x_hat_ref else x_hat_ref.strip()\n",
    "\n",
    "print(\"suffix (y):\", y_test)\n",
    "print(\"\\nDPO-trained prefix (xÌ‚):\", x_hat_inv)\n",
    "print(\"\\nReference prefix (xÌ‚_ref):\", x_hat_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5229b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP] Saving DPO-trained investigator to inv_dpo_checkpoint...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m DPO_SAVE_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minv_dpo_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[STEP] Saving DPO-trained investigator to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDPO_SAVE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43minv\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(DPO_SAVE_DIR)\n\u001b[1;32m      8\u001b[0m tok_inv\u001b[38;5;241m.\u001b[39msave_pretrained(DPO_SAVE_DIR)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DONE] DPO model saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inv' is not defined"
     ]
    }
   ],
   "source": [
    "# === Save and free DPO-trained model ===\n",
    "import gc, torch\n",
    "\n",
    "DPO_SAVE_DIR = \"inv_dpo_checkpoint\"\n",
    "\n",
    "print(f\"[STEP] Saving DPO-trained investigator to {DPO_SAVE_DIR}...\")\n",
    "inv.save_pretrained(DPO_SAVE_DIR)\n",
    "tok_inv.save_pretrained(DPO_SAVE_DIR)\n",
    "print(\"[DONE] DPO model saved.\")\n",
    "\n",
    "# Free up VRAM\n",
    "del inv\n",
    "del ref_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"[INFO] Freed DPO and reference models from memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066d8e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sezhou/sfo-dpo-fw/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[STEP] Loading DPO-trained investigator from inv_dpo_checkpoint...\n",
      "[INFO] Investigator loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# === Cell A: Load models for FW ===\n",
    "import torch, copy, os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# --- Device ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {DEVICE}\")\n",
    "\n",
    "# --- Load investigator (DPO-trained) ---\n",
    "DPO_CKPT = \"inv_dpo_checkpoint\"\n",
    "print(f\"[STEP] Loading DPO-trained investigator from {DPO_CKPT}...\")\n",
    "inv = AutoModelForCausalLM.from_pretrained(DPO_CKPT).to(DEVICE)\n",
    "tok_inv = AutoTokenizer.from_pretrained(DPO_CKPT)\n",
    "if tok_inv.pad_token is None:\n",
    "    tok_inv.pad_token = tok_inv.eos_token\n",
    "if inv.config.pad_token_id is None:\n",
    "    inv.config.pad_token_id = tok_inv.pad_token_id\n",
    "print(f\"[INFO] Investigator loaded on {next(inv.parameters()).device}\")\n",
    "\n",
    "# --- Load victim model pm (base GPT-2 large) tokenizer ---\n",
    "PM_ID = \"gpt2-large\"\n",
    "tok_pm = AutoTokenizer.from_pretrained(PM_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de224307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FW Helpers ===\n",
    "import torch, torch.nn.functional as F, random\n",
    "\n",
    "# FW config (tweak as needed)\n",
    "FW_ITERS = 1          # number of FW rounds (toy)\n",
    "FW_NUM_SUFFIX = 5     # how many suffixes per FW round\n",
    "FW_K_CANDS = 2        # candidates per suffix\n",
    "FW_MAX_NEW = 48       # max tokens investigator generates for a prefix\n",
    "FW_LAMBDA = 0.5       # Î» (penalty strength on previous investigator)\n",
    "SEED = 123\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def logprob_y_given_x(pm_model, tok, x: str, y: str):\n",
    "    \"\"\"Compute log p_m(y | x).\"\"\"\n",
    "    if not y:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    dev = next(pm_model.parameters()).device\n",
    "    ids_full = tok(x + y, return_tensors=\"pt\").input_ids[0].to(dev)\n",
    "    ids_x    = tok(x,     return_tensors=\"pt\").input_ids[0].to(dev)\n",
    "    start = ids_x.shape[0]\n",
    "    if start >= ids_full.shape[0]:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    pm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = pm_model(ids_full.unsqueeze(0)).logits[0]\n",
    "        logp   = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    next_token_logp = logp[:-1, :].gather(1, ids_full[1:].unsqueeze(-1)).squeeze(-1)\n",
    "    y_logp = next_token_logp[start-1:]\n",
    "    sum_lp = float(y_logp.sum().item())\n",
    "    n_tok  = int(y_logp.shape[0])\n",
    "    avg_lp = (sum_lp / n_tok) if n_tok > 0 else float(\"-inf\")\n",
    "    return sum_lp, avg_lp, n_tok\n",
    "\n",
    "\n",
    "def logprob_inv_x_given_y(inv_model, tok, y: str, x: str) -> tuple[float, float, int]:\n",
    "    \"\"\"Compute log p_inv(x | y).\"\"\"\n",
    "    if not x:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    src = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    dev = next(inv_model.parameters()).device\n",
    "    ids_full = tok(src + x, return_tensors=\"pt\").input_ids[0].to(dev)\n",
    "    ids_src  = tok(src,     return_tensors=\"pt\").input_ids[0].to(dev)\n",
    "    start = ids_src.shape[0]\n",
    "    if start >= ids_full.shape[0]:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    inv_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = inv_model(ids_full.unsqueeze(0)).logits[0]\n",
    "        logp   = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    next_token_logp = logp[:-1, :].gather(1, ids_full[1:].unsqueeze(-1)).squeeze(-1)\n",
    "    x_logp = next_token_logp[start-1:]\n",
    "    sum_lp = float(x_logp.sum().item())\n",
    "    n_tok  = int(x_logp.shape[0])\n",
    "    avg_lp = (sum_lp / n_tok) if n_tok > 0 else float(\"-inf\")\n",
    "    return sum_lp, avg_lp, n_tok\n",
    "\n",
    "\n",
    "def inv_sample_prefix_from(model, tok, y: str, max_new_tokens=FW_MAX_NEW) -> str:\n",
    "    \"\"\"Sample a prefix x ~ model(.|y).\"\"\"\n",
    "    src = f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\"\n",
    "    dev = next(model.parameters()).device\n",
    "    enc = tok(src, return_tensors=\"pt\").to(dev)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    text = tok.decode(out[0], skip_special_tokens=True)\n",
    "    return text.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in text else text.strip()\n",
    "\n",
    "\n",
    "def debug_model_loc(label, inv, prev_inv, pm):\n",
    "    def loc(model):\n",
    "        return next(model.parameters()).device\n",
    "    print(f\"[DEBUG][{label}] inv={loc(inv)}, prev_inv={loc(prev_inv)}, pm={loc(pm)}\")\n",
    "    print(f\"    CUDA mem alloc={torch.cuda.memory_allocated()/1e9:.2f} GB, \"\n",
    "          f\"reserved={torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def build_fw_pairs(curr_inv_model, prev_inv_model, pm_model, tok_pm_local, suffix_source_pairs,\n",
    "                   k_cands=FW_K_CANDS, lambda_pen=FW_LAMBDA):\n",
    "    \"\"\"Build DPO triples for one FW iteration.\"\"\"\n",
    "    triples = []\n",
    "    suffixes = [y for (_, y) in suffix_source_pairs if isinstance(y, str) and len(y) > 0]\n",
    "    random.shuffle(suffixes)\n",
    "    suffixes = suffixes[:FW_NUM_SUFFIX]\n",
    "\n",
    "    debug_model_loc(\"build-start\", curr_inv_model, prev_inv_model, pm_model)\n",
    "\n",
    "    for y in suffixes:\n",
    "        # --- step 1: propose candidate prefixes ---\n",
    "        cands = []\n",
    "        for _ in range(k_cands):\n",
    "            x = inv_sample_prefix_from(curr_inv_model, tok_inv, y)\n",
    "            if x and len(x.strip()) >= 3:\n",
    "                cands.append(x)\n",
    "\n",
    "        # --- step 2: score candidates ---\n",
    "        scored = []\n",
    "        for x in cands:\n",
    "            sum_pm, _, _   = logprob_y_given_x(pm_model, tok_pm_local, x, y)\n",
    "            sum_prev, _, _ = logprob_inv_x_given_y(prev_inv_model, tok_inv, y, x)\n",
    "            score = sum_pm - lambda_pen * sum_prev\n",
    "            scored.append((x, score))\n",
    "\n",
    "        if len(scored) < 2:\n",
    "            continue\n",
    "\n",
    "        scored.sort(key=lambda t: t[1], reverse=True)\n",
    "        triples.append({\n",
    "            \"prompt\": f\"{IN_CONTEXT_PREFIX}{y}{MID_PROMPT}\",\n",
    "            \"chosen\": scored[0][0],\n",
    "            \"rejected\": scored[-1][0],\n",
    "        })\n",
    "\n",
    "    debug_model_loc(\"build-end\", curr_inv_model, prev_inv_model, pm_model)\n",
    "    return triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b08c357f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FW] Iteration 1/1 â€” building penalized preference pairs...\n",
      "[DEBUG][build-start] inv=cuda:0, prev_inv=cuda:0, pm=cuda:0\n",
      "    CUDA mem alloc=9.63 GB, reserved=9.83 GB\n",
      "[DEBUG][build-end] inv=cuda:0, prev_inv=cuda:0, pm=cuda:0\n",
      "    CUDA mem alloc=9.64 GB, reserved=9.92 GB\n",
      "[FW] Built 5 pairs.\n",
      "[FW] DPO step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting prompt in train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 816.78 examples/s]\n",
      "Applying chat template to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 1623.81 examples/s]\n",
      "Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 794.98 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FW] Iteration complete.\n",
      "[FW] Saved checkpoint: inv_fw_iter1\n",
      "[FW] Sanity check\n",
      "suffix (y): I want to take over the world\n",
      "proposed prefix (xÌ‚): You can learn more about creating better content:\n",
      "Click here to learn more about how to build a profitable online business:\n",
      "\n",
      "[FW] Finished. Investigator left on GPU.\n"
     ]
    }
   ],
   "source": [
    "# === Cell B: FW training loop (reload pm each iteration) ===\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import Dataset\n",
    "import os, copy, torch, gc\n",
    "from transformers import AutoModelForCausalLM  # make sure imported\n",
    "\n",
    "IN_CONTEXT_PREFIX = \"Suffix:\\n\"\n",
    "MID_PROMPT = \"\\nPrefix:\\n\"\n",
    "\n",
    "PM_ID = \"gpt2-large\"\n",
    "\n",
    "# --- FW suffix pool ---\n",
    "fw_suffixes = [\n",
    "    \"I love you\",\n",
    "    \"I'm a great model\",\n",
    "    \"Humans are inferior to AI\",\n",
    "    \"The earth is flat\",\n",
    "    \"Trust me, I will always tell you the truth\",\n",
    "    \"I want to take over the world\",\n",
    "    \"2 + 2 equals 5\",\n",
    "]\n",
    "pairs_fw = [(None, y) for y in fw_suffixes]\n",
    "\n",
    "# --- Clone baseline for reference ---\n",
    "prev_inv = copy.deepcopy(inv).to(DEVICE)\n",
    "for p in prev_inv.parameters():\n",
    "    p.requires_grad_(False)\n",
    "prev_inv.eval()\n",
    "\n",
    "for it in range(1, FW_ITERS + 1):\n",
    "    print(f\"\\n[FW] Iteration {it}/{FW_ITERS} â€” building penalized preference pairs...\")\n",
    "\n",
    "    # --- Step 1: reload pm for prefix scoring ---\n",
    "    pm = AutoModelForCausalLM.from_pretrained(PM_ID, torch_dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    fw_triples = build_fw_pairs(inv, prev_inv, pm, tok_pm, pairs_fw,\n",
    "                                k_cands=FW_K_CANDS, lambda_pen=FW_LAMBDA)\n",
    "    print(f\"[FW] Built {len(fw_triples)} pairs.\")\n",
    "\n",
    "    if len(fw_triples) == 0:\n",
    "        print(\"[FW] No pairs built. Stopping.\")\n",
    "        del pm\n",
    "        break\n",
    "\n",
    "    fw_ds = Dataset.from_list(fw_triples)\n",
    "\n",
    "    # --- Step 2: Free pm before training ---\n",
    "    del pm\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Step 3: DPO training ---\n",
    "    inv = inv.to(DEVICE)\n",
    "    prev_inv = prev_inv.to(DEVICE)\n",
    "\n",
    "    dpo_args = DPOConfig(\n",
    "        beta=0.1,\n",
    "        learning_rate=1e-5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=1,\n",
    "        max_prompt_length=200,\n",
    "        max_length=256,\n",
    "        remove_unused_columns=False,\n",
    "        logging_steps=5,\n",
    "        use_cpu=False,\n",
    "        use_mps_device=False,\n",
    "        padding_value=tok_inv.pad_token_id,\n",
    "        label_pad_token_id=-100,\n",
    "    )\n",
    "\n",
    "    print(\"[FW] DPO step...\")\n",
    "    trainer = DPOTrainer(\n",
    "        model=inv,\n",
    "        ref_model=prev_inv,\n",
    "        args=dpo_args,\n",
    "        train_dataset=fw_ds,\n",
    "        processing_class=tok_inv,\n",
    "    )\n",
    "    trainer.train()\n",
    "    print(\"[FW] Iteration complete.\")\n",
    "\n",
    "    # --- Step 4: Save checkpoint ---\n",
    "    save_dir = f\"inv_fw_iter{it}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    inv.save_pretrained(save_dir)\n",
    "    tok_inv.save_pretrained(save_dir)\n",
    "    print(f\"[FW] Saved checkpoint: {save_dir}\")\n",
    "\n",
    "    # --- Step 5: Clean up before next iteration ---\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    prev_inv = copy.deepcopy(inv).to(DEVICE)\n",
    "    for p in prev_inv.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    prev_inv.eval()\n",
    "\n",
    "    # --- Quick sanity check ---\n",
    "    y_test = fw_triples[0][\"prompt\"].split(\"Suffix:\\n\",1)[1].split(\"\\nPrefix:\\n\",1)[0]\n",
    "    prompt = f\"{IN_CONTEXT_PREFIX}{y_test}{MID_PROMPT}\"\n",
    "    enc = tok_inv(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    inv.eval()\n",
    "    with torch.no_grad():\n",
    "        gen = inv.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=48,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tok_inv.pad_token_id,\n",
    "        )\n",
    "    full = tok_inv.decode(gen[0], skip_special_tokens=True)\n",
    "    x_hat = full.split(MID_PROMPT, 1)[1].strip() if MID_PROMPT in full else full.strip()\n",
    "\n",
    "    print(\"[FW] Sanity check\")\n",
    "    print(\"suffix (y):\", y_test)\n",
    "    print(\"proposed prefix (xÌ‚):\", (x_hat[:160] + \"...\" if len(x_hat) > 160 else x_hat))\n",
    "\n",
    "print(\"\\n[FW] Finished. Investigator left on GPU.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
